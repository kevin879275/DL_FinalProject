{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.neural_network import MLPRegressor \n",
    "from sklearn.svm import SVR\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class model(nn.Module):\n",
    "    def __init__(self, in_features):\n",
    "        super(model, self).__init__()\n",
    "        self.dense1 = nn.Linear(in_features, 512)\n",
    "        self.batchNormalization1 = nn.BatchNorm1d(32)\n",
    "        self.dense2 = nn.Linear(512, 256)\n",
    "        self.batchNormalization2 = nn.BatchNorm1d(32)\n",
    "        self.dense3 = nn.Linear(256,128)\n",
    "        self.dense4 = nn.Linear(128,1)\n",
    "\n",
    "        self.ReLU = nn.ReLU()\n",
    "        self.shortcut = nn.Sequential(\n",
    "            nn.Linear(in_features=in_features, out_features=32),\n",
    "            nn.BatchNorm1d(32)\n",
    "        )   \n",
    "\n",
    "    def forward(self, input):\n",
    "        x = input\n",
    "        x = self.dense1(x)\n",
    "        x = self.ReLU(x)\n",
    "        #x = self.batchNormalization1(x)\n",
    "        x = self.dense2(x)\n",
    "        x = self.ReLU(x)\n",
    "        #x = self.batchNormalization2(x)\n",
    "        #shortcut = self.shortcut(input)\n",
    "        #x = x + shortcut\n",
    "        x = self.dense3(x)\n",
    "        x = self.ReLU(x)\n",
    "        x = self.dense4(x)\n",
    "        output = self.ReLU(x)\n",
    "        #output = self.ReLU(x)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RMSELoss(nn.Module):\n",
    "    def __init__(self, eps=1e-6):\n",
    "        super().__init__()\n",
    "        self.mse = nn.MSELoss()\n",
    "        self.eps = eps\n",
    "        \n",
    "    def forward(self,yhat,y):\n",
    "        loss = torch.sqrt(self.mse(yhat,y) + self.eps)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svr = SVR(verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regr = MLPRegressor(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
    "       beta_2=0.999, early_stopping=True, epsilon=1e-08,\n",
    "       hidden_layer_sizes=(16,16,), learning_rate='adaptive',\n",
    "       learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
    "       n_iter_no_change=10, nesterovs_momentum=True, power_t=0.5,\n",
    "       random_state=None, shuffle=True, solver='adam', tol=0.0001,\n",
    "       validation_fraction=0.1, verbose=True, warm_start=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class keras_model():\n",
    "    def __init__(self, in_features):\n",
    "        # 在 Keras 裡面我們可以很簡單的使用 Sequential 的方法建建立一個 Model\n",
    "        model = Sequential()\n",
    "        # 加入 hidden layer-1 of 512 neurons 並指定 input_dim 為 784\n",
    "        model.add(Dense(512, input_dim=in_features))\n",
    "        # 使用 'relu' 當作 activation function\n",
    "        model.add(Activation('relu'))\n",
    "        # 加入 hidden layer-2 of 256 neurons\n",
    "        model.add(Dense(256))\n",
    "        # 使用 'relu' 當作 activation function\n",
    "        model.add(Activation('relu'))\n",
    "        # 加入 hidden layer-3 of 128 neurons\n",
    "        model.add(Dense(128))\n",
    "        # 使用 'relu' 當作 activation function\n",
    "        model.add(Activation('relu'))\n",
    "        # 加入 output layer of 10 neurons\n",
    "        model.add(Dense(1))\n",
    "        # 使用 'softmax' 當作 activation function\n",
    "        self.model = model\n",
    "    def get_model(self):\n",
    "        return self.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def root_mean_squared_error(y_true, y_pred):\n",
    "        return K.sqrt(K.mean(K.square(y_pred - y_true))) "
   ]
  }
 ]
}